---
title: "Introduction to StatComp20056(two functions)"
author: "20056"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp20056(two functions)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Function 1


```{r}
library(kedd)
epan<-function(x){
  (3/4)*(1-x^2)*(abs(x)<=1)
}
esti_multi<-function(x,y,hx=0.5,hy=0.5,mat,method=c("multi","sphe")){
  n<-nrow(mat)
  temp<-numeric(n)
  if(method=="multi"){
    for(i in 1:n){
      temp[i]<-epan((x-mat[i,1])/hx)*epan((y-mat[i,2])/hy)/(hx*hy)
    }
    return(mean(temp))
  }
  if(method=="sphe"){
    for(i in 1:n){
      u<-(x-mat[i,1])/hx
      v<-(y-mat[i,2])/hy
      temp[i]<-(1-(u^2+v^2))*((u^2+v^2)<=1)/(pi/2)/(hx*hy)
    }
    return(mean(temp))
  }
}
attach(faithful)
mat1<-cbind(eruptions,waiting)
n<-nrow(mat1)
xlab<-seq(0,6,0.1)
ylab<-seq(40,100,1)
z_density<-matrix(0,length(xlab),length(ylab))
for(i in 1:length(xlab)){
for(j in 1:length(ylab)){
  z_density[i,j]<-esti_multi(x=xlab[i],y=ylab[j],mat=mat1,method = "multi")
  }}
persp(xlab, ylab,z_density,phi = 30, theta = 40, col ="blue", border = 0, 
zlim = c(0, 0.08),zlab = "Multiplicative KDE")
```

The function **esti_multi** is to obtain the kernel estimation of binary joint density.
Epanechnikov kernel function is $k(x)=\frac{3}{4}(1-x^2), x\leq 1$,which can be used in univariate kenerl density estimation. In the case of multiple variables, two common methods for selecting multiple kernel functions are multiplicative kernel and spherical-symmetric kernel.

The multivariate Epanechnikov (spherical):
$$
\mathcal{K}(\mathbf{u}) \propto\left(1-\mathbf{u}^{\prime} \mathbf{u}\right) I\left(\mathbf{u}^{\prime} \mathbf{u} \leq 1\right)
$$
The multivariate Epanechnikov (multiplicative):
$$
\mathcal{K}(\mathbf{u})=\left(\frac{3}{4}\right)^{d} \prod_{j=1}^{d}\left(1-u_{j}^{2}\right) I\left(\left|u_{j}\right| \leq 1\right)
$$
It is not difficult to prove that the first spherical kernel has integeral of $\pi/2$.

## Function 2

```{r}
Gibbs_samp<-function(n=3000,x1,x2,plot=c("T","F"),paras=c(1,2,1,2,0.5)){
  mu1<-paras[1];mu2<-paras[2];sigma1<-paras[3];sigma2<-paras[4];ro<-paras[5]
  m<-1000
  mat<-matrix(0,n+m+1,2)
  mat[1,]<-c(x1,x2)
  for(i in 2:n+1){
    x2<-mat[i-1,2]
    x1<-rnorm(1,mu1+ro*sigma1/sigma2*(x2-mu2),sqrt(1-ro^2)*sigma1)
    mat[i,1]<-x1
    x2<-rnorm(1,mu2+ro*sigma2/sigma1*(x1-mu1),sqrt(1-ro^2)*sigma2)
    mat[i,2]<-x2
  }
  mat1<-mat[-(1:(m+1)),]
  mu<-apply(mat1, 2,mean)
  sigma<-cov(mat1)
  corr<-cor(mat1)[1,2]
  if(plot=="T"){
    plot(mat1[,1],mat1[,2])
  }
  return(list(chain=mat1,mu=mu,sigma=sigma,corr=corr))
}
out1<-Gibbs_samp(3000,0,0)
out1
out2<-Gibbs_samp(1000,0,0,plot="F")
out2
```

This function **Gibbs_samp** is a Gibbs sampler for Bivariate distribution.Given the required sample size, initial value, mean variance, correlation coefficient and other parameters of the target binary distribution, we can get a sample that follows the target distribution smoothly.

1. Initialize $X(0)$ at time $t=0$.

2. For each iteration, indexed $t=1,2, \ldots$ repeat:

(a) Set $x_{1}=X_{1}(t-1)$

(b) For each coordinate $j=1, \ldots, d$

(a) Generate $X_{j}^{*}(t)$ from $f\left(X_{j} \mid x_{(-j)}\right)$

(b) Update $x_{j}=X_{j}^{*}(t)$

(c) Set $X(t)=\left(X_{1}^{*}(t), \ldots, X_{d}^{*}(t)\right)$ (every candidate is accepted).

(d) Increment $t$.

The above is the general form of Gibbs sampler algorithm. The following is the specific conditions that depend on the generation of bivariate normal distribution samples.

$$
\begin{array}{c}
E\left[X_{2} \mid x_{1}\right]=\mu_{1}+\rho \frac{\sigma_{2}}{\sigma_{1}}\left(x_{1}-\mu_{1}\right) \\
\operatorname{Var}\left(X_{2} \mid x_{1}\right)=\left(1-\rho^{2}\right) \sigma_{2}^{2}
\end{array}
$$
and the chain is generated by sampling from
$$
\begin{array}{l}
f\left(x_{1} \mid x_{2}\right) \sim \operatorname{Normal}\left(\mu_{1}+\frac{\rho \sigma_{1}}{\sigma_{2}}\left(x_{2}-\mu_{2}\right),\left(1-\rho^{2}\right) \sigma_{1}^{2}\right) \\
f\left(x_{2} \mid x_{1}\right) \sim \operatorname{Normal}\left(\mu_{2}+\frac{\rho \sigma_{2}}{\sigma_{1}}\left(x_{1}-\mu_{1}\right),\left(1-\rho^{2}\right) \sigma_{2}^{2}\right)
\end{array}
$$

In this function, we burn the previous m(=1000) samples to obtain the samples that follow the stationary target distribution.
